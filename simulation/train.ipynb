{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-implement-keras-layers-core-lambda-in-pytorch/5903\n",
    "class BestMeanPooling(nn.Module):\n",
    "    \n",
    "  def __init__(self, n_topN):\n",
    "    super(BestMeanPooling, self).__init__()\n",
    "    self.topN = n_topN\n",
    "        \n",
    "  def forward(self, aX_tr):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    aX_tr_sorted, _ = torch.sort(aX_tr, axis=1) # also return index\n",
    "    return torch.mean(aX_tr_sorted[:, -self.topN:,:], axis=1)\n",
    "    # print(\"B = \", aX_tr_sorted.size(), aX_tr_sorted)\n",
    "    # c = torch.mean(aX_tr_sorted[:, -self.topN:,:], axis=1)\n",
    "    # print(\"C = \", c.size(), c)\n",
    "    return c\n",
    "\n",
    "# Test\n",
    "aA = torch.randn(1, 200).reshape(4, 10, 5)\n",
    "mean_pool = BestMeanPooling(2) \n",
    "aA = mean_pool(aA)\n",
    "# print(aA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticData(Dataset):\n",
    "    def __init__(self, upper, x, y, z):\n",
    "      self.aX_tr_sy = np.random.random_sample(upper).reshape(x, y, z)\n",
    "      self.ay_tr_sy = np.random.randint(0, 2, x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.aX_tr_sy)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.aX_tr_sy[idx], self.ay_tr_sy[idx] #.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassCNN(nn.Module):\n",
    "\n",
    "  def __init__(self, n_in, n_out, num_event=10, topN=5, prob=0.00):\n",
    "    super(MassCNN,self).__init__()\n",
    "    self.n_filter = n_out\n",
    "\n",
    "    #first layers\n",
    "    self.conv_1        = nn.Conv1d(n_in, n_out, kernel_size=1)\n",
    "    \n",
    "    # output of the layer is # of filters\n",
    "    self.bmpool_1      = BestMeanPooling(topN)\n",
    "    self.dropout_L1    = nn.Dropout(p=prob)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    self.fcn_1 = nn.Linear(3, 2)\n",
    "    # self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, aX_tr):\n",
    "    aX_tr = self.conv_1(aX_tr)\n",
    "    aX_tr = torch.relu(aX_tr)\n",
    "    \n",
    "    # This will collapse one dimension\n",
    "    aX_tr = self.bmpool_1(aX_tr)\n",
    "    \n",
    "    if self.n_filter > 4: \n",
    "      aX_tr = self.dropout_L1(aX_tr)\n",
    "    \n",
    "    aX_tr = aX_tr.view(aX_tr.size(0), -1)\n",
    "    aX_tr = self.fcn_1(aX_tr)\n",
    "    # aX_tr = self.softmax(aX_tr)\n",
    "    return aX_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs=10, lr=0.1, filter=12, maxpool_percent=100.00, n_iter = 0,\n",
    "          check_point=False):\n",
    "    \"\"\"\n",
    "      Train the model with Monte Carlo sampling in which hyperparameters\n",
    "      are randomly generated. Multiple simulations are run to obtain \n",
    "      statistically robust models.\n",
    "\n",
    "      params:\n",
    "\n",
    "      returns:\n",
    "        # of best solutions (models)\n",
    "    \"\"\"\n",
    "    # Random data\n",
    "    d_trains = SyntheticData(750, 5, 50, 3)\n",
    "    d_valids = SyntheticData(750, 5, 50, 3)\n",
    "   \n",
    "    t_cost_per_epoch = []\n",
    "    \n",
    "    # model.state_dict()['cnn1.weight']\n",
    "    # model.state_dict()['cnn2.weight']\n",
    "    af_Tr = DataLoader(d_trains, batch_size=100, shuffle=True)\n",
    "    af_Va = DataLoader(d_valids, batch_size=10, shuffle=True)\n",
    "    t_solutions = []\n",
    "    for epoch in range(n_epochs):\n",
    "      # model, optimizer, and lost function\n",
    "      o_mass_model = MassCNN(50, filter, 3, 5)\n",
    "      o_optimizer = torch.optim.SGD(o_mass_model.parameters(), lr=0.0001)\n",
    "      o_cost      = torch.nn.CrossEntropyLoss()\n",
    "      m_models = dict()\n",
    "      fgr_cost = 0\n",
    "      for af_X, af_y in af_Tr:  # iterative over batch of data\n",
    "        o_optimizer.zero_grad() ## Back propagation the loss\n",
    "        af_y_pr  = o_mass_model(af_X.float())\n",
    "        o_loss   = o_cost(af_y_pr, af_y)\n",
    "        o_loss.backward()\n",
    "        o_optimizer.step() # back propagate\n",
    "        fgr_cost += o_loss.data\n",
    "      t_cost_per_epoch.append(fgr_cost)\n",
    "    \n",
    "      #print(o_mass_model.state_dict().keys())\n",
    "      #print(\"Epochs in fun \", epoch, id(o_mass_model))\n",
    "      # Where is validation loss?\n",
    "      o_mass_model.eval()\n",
    "      f_correct = 0\n",
    "      for af_x_va, af_y_va in af_Va:\n",
    "        z  = o_mass_model(af_x_va.float()).data\n",
    "        _, yhat = torch.max(z.data, 1)\n",
    "        f_correct += (yhat == af_y_va).sum().item()\n",
    "      f_accuracy = \"%0.3f\" %(f_correct / 1000) # fixed this with sample size\n",
    "    \n",
    "      m_models[\"epoch\"] = epoch \n",
    "      m_models[\"iteration\"] = n_iter\n",
    "      m_models[\"val_acc\"] = f_accuracy\n",
    "      m_models[\"model_state\"] =  o_mass_model.state_dict()\n",
    "      m_models[\"optimizer_state\"] =  o_optimizer.state_dict()\n",
    "      m_models[\"loss\"] =  o_cost.state_dict()\n",
    "      m_models[\"model\"] = o_mass_model\n",
    "      t_solutions.append(m_models.copy())\n",
    "      del m_models\n",
    "    return t_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(t_models):\n",
    "  \"\"\"\n",
    "  Select the best solutions from the pool\n",
    "  \n",
    "  params: \n",
    "    t_models: list of models. Each model is dictionary containing parameters\n",
    "              and hyperparameters.\n",
    "  return: \n",
    "    list of best models\n",
    "  \"\"\"\n",
    "  n_best_sol      = 5\n",
    "  t_best_val_accs = []\n",
    "  t_best_models   = []\n",
    "    \n",
    "  for o_model in t_models: \n",
    "    b_update  = True\n",
    "    f_val_acc = float(o_model[\"val_acc\"])\n",
    "    if len(t_best_val_accs) >= n_best_sol:\n",
    "      print(t_best_val_accs)\n",
    "      f_lowest_acc = np.min(t_best_val_accs)\n",
    "      if f_val_acc > f_lowest_acc:\n",
    "        i_cur_idx = t_best_val_accs.index(f_lowest_acc)\n",
    "        t_best_val_accs.pop(i_cur_idx)\n",
    "        t_best_models.pop(i_cur_idx)\n",
    "      else:\n",
    "        b_update = False\n",
    "    if b_update: \n",
    "      t_best_val_accs.append(f_val_acc)\n",
    "      t_best_models.append(o_model)\n",
    "  return t_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the multiple models for testing\n",
    "\n",
    "def test_model(d_tests, t_best_sols):\n",
    "  \"\"\"\n",
    "  Test performance of model on independent data. Since multiple models were generated, \n",
    "  these models were tested aganist an independent data and their average performance \n",
    "  will be returned as final result.\n",
    "  \n",
    "  params:\n",
    "    d_tests: data for independent test\n",
    "    t_best_sols: list of best solutions\n",
    "  \"\"\"\n",
    "  t_test_accs = []\n",
    "  for o_best_sol in t_best_sols:\n",
    "    o_best_model = o_best_sol[\"model\"]\n",
    "    o_best_model.eval()\n",
    "    f_correct = 0\n",
    "    n_test_count = len(d_tests)\n",
    "    for d_test_X, d_target_Y in d_tests:\n",
    "      z  = o_best_model(d_test_X.float()).data # remove float\n",
    "      _, yhat = torch.max(z.data, 1)\n",
    "      f_correct += (yhat == d_target_Y).sum().item()\n",
    "    f_accuracy = \"%0.3f\" %(f_correct / n_test_count) # fixed this with sample size\n",
    "    t_test_accs.append(float(f_accuracy))\n",
    "  print(\"Coding: \", t_test_accs)\n",
    "  return np.mean(t_test_accs)\n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "[0.002, 0.002, 0.002, 0.002, 0.003]\n",
      "[0.002, 0.002, 0.002, 0.003, 0.003]\n",
      "[0.002, 0.002, 0.003, 0.003, 0.003]\n",
      "[0.002, 0.002, 0.003, 0.003, 0.003]\n",
      "[0.002, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.002, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.004]\n",
      "[0.003, 0.003, 0.003, 0.004, 0.004]\n",
      "[0.003, 0.003, 0.003, 0.004, 0.004]\n",
      "[0.003, 0.003, 0.003, 0.004, 0.004]\n",
      "[0.003, 0.003, 0.003, 0.004, 0.004]\n",
      "[0.003, 0.003, 0.003, 0.004, 0.004]\n",
      "[0.003, 0.003, 0.003, 0.004, 0.004]\n",
      "[0.003, 0.003, 0.004, 0.004, 0.004]\n",
      "[0.003, 0.003, 0.004, 0.004, 0.004]\n",
      "[0.003, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.003, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "Coding:  [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "Coding:  0.2\n"
     ]
    }
   ],
   "source": [
    "# get 20% of best solutions after iterating \n",
    "# between 15 to 30 times (these # are randomly selected)\n",
    "\n",
    "n_cell = 10\n",
    "f_maxpools    = [0.01, 1.0, 5.0, 20.0, 100.0]\n",
    "n_maxpool_len = len(f_maxpools)\n",
    "\n",
    "t_models      = []\n",
    "for n_trial in range(10):\n",
    "  f_lr       = 10 ** np.random.uniform(-3, -2)\n",
    "  i_filter   = np.random.choice(range(3,10))\n",
    "  f_max_pool = f_maxpools[n_trial%n_maxpool_len]\n",
    "  f_maxpool  = max(1, int(f_max_pool/100. * n_cell))\n",
    "  t_models  += train(10, f_lr, i_filter, f_maxpool)\n",
    "  # if b_checkpoint: # save check point files\n",
    "    \n",
    "t_best_models = get_best_models(t_models)\n",
    "\n",
    "# generate test data\n",
    "d_tests  = SyntheticData(750, 5, 50, 3)\n",
    "af_test  = DataLoader(d_tests)\n",
    "predicted_Y = test_model(af_test, t_best_models)\n",
    "print(\"Coding: \", predicted_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 5)\n",
      "(None, 20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Identity:0' shape=(None, 20, 10) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Activation, Dropout\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# the input layer\n",
    "ncell = 20\n",
    "nmark = 5\n",
    "data_input = Input(shape=(ncell, nmark))\n",
    "\n",
    "nfilter = 10\n",
    "# the filters\n",
    "print(data_input.shape)\n",
    "conv = Convolution1D(nfilter, 1, activation='linear', name='conv1')(data_input)\n",
    "print(conv.shape)\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "K.print_tensor(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, stride=1, padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4])\n",
      "torch.Size([5, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "in_channel = 16\n",
    "out_channel = 33\n",
    "filter/kernal = 3\n",
    "batch = 20\n",
    "\"\"\"\n",
    "\n",
    "m = nn.Conv1d(3, 6, 2, stride=1)\n",
    "input = torch.randn(5, 3, 4)\n",
    "print(input.shape)\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0291e+00, -3.4732e-01, -1.9905e+00,  2.9125e-01],\n",
       "         [-1.6340e+00,  2.1681e-01,  7.8561e-01, -6.0226e-01],\n",
       "         [ 8.7316e-01, -2.3531e-01, -1.0035e+00,  4.2327e-01]],\n",
       "\n",
       "        [[-5.8480e-01, -5.1901e-01,  1.3230e-01,  1.5052e+00],\n",
       "         [ 8.2028e-01,  1.4783e+00, -9.7886e-01,  4.1739e-01],\n",
       "         [-3.1428e-01, -8.5926e-01, -2.6044e-01,  7.8873e-01]],\n",
       "\n",
       "        [[ 1.2651e+00,  8.4648e-01, -9.4190e-01,  6.6988e-01],\n",
       "         [ 7.3282e-01,  1.4394e-01,  4.8916e-04,  1.6228e+00],\n",
       "         [-1.1017e+00, -3.8403e-01, -1.1204e-01,  2.3949e-01]],\n",
       "\n",
       "        [[-1.3718e+00, -1.4614e+00, -8.7649e-02,  1.8829e+00],\n",
       "         [-5.6087e-01,  1.1582e+00, -6.7663e-01, -1.1620e+00],\n",
       "         [-5.0400e-01, -6.1528e-01, -8.3913e-01,  3.6212e-01]],\n",
       "\n",
       "        [[-1.3789e+00,  4.6552e-01, -4.7711e-01,  2.3987e-03],\n",
       "         [ 2.2687e+00,  3.3071e-01, -6.9713e-01, -6.7381e-01],\n",
       "         [-8.1790e-01, -2.4145e+00, -9.2545e-01, -1.6154e+00]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3641,  0.7343,  0.3518],\n",
       "         [ 0.5452, -0.3405,  0.2656],\n",
       "         [ 0.2508,  0.1945, -0.6212],\n",
       "         [ 0.8256,  0.9600,  0.2388],\n",
       "         [ 0.0887, -0.1202,  0.8158],\n",
       "         [ 0.6137,  0.2650, -0.1434]],\n",
       "\n",
       "        [[ 0.5647, -0.6174,  0.2952],\n",
       "         [-0.2418, -0.1800,  0.4797],\n",
       "         [ 0.2544, -0.7770, -0.1015],\n",
       "         [ 0.3122,  0.2029, -0.2616],\n",
       "         [ 0.1012,  0.6687,  0.5326],\n",
       "         [-0.3271,  0.2342, -0.0328]],\n",
       "\n",
       "        [[-0.7463, -0.1336,  0.8619],\n",
       "         [-0.4816, -0.5939,  0.1808],\n",
       "         [-0.5614, -0.0674,  0.4098],\n",
       "         [-0.4515,  0.3491, -0.0699],\n",
       "         [ 0.3950, -0.0567,  0.2680],\n",
       "         [ 0.2312,  0.5782, -0.5729]],\n",
       "\n",
       "        [[ 1.5025, -0.0068, -0.2961],\n",
       "         [ 0.1962,  0.4127,  0.9048],\n",
       "         [ 0.1880, -0.7258, -1.0428],\n",
       "         [ 0.8816,  0.6617, -0.0872],\n",
       "         [ 0.1281,  0.8463,  1.1389],\n",
       "         [-0.0788,  0.0059,  0.2821]],\n",
       "\n",
       "        [[-0.2409, -0.2157,  0.3521],\n",
       "         [ 0.4806, -0.4048,  1.0856],\n",
       "         [-0.8271, -1.2934, -1.0841],\n",
       "         [ 0.4911,  0.0547,  0.9081],\n",
       "         [ 1.0342,  0.6199,  1.0287],\n",
       "         [-0.5116,  0.5995,  0.3836]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item  (1, 'J') heap  [(1, 'J')]\n",
      "item  (4, 'N') heap  [(1, 'J'), (4, 'N')]\n",
      "item  (3, 'H') heap  [(1, 'J'), (4, 'N'), (3, 'H')]\n",
      "item  (2, 'O') heap  [(1, 'J'), (2, 'O'), (3, 'H'), (4, 'N')]\n",
      "J\n",
      "O\n",
      "H\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "from heapq import heappush, heappop\n",
    "heap = []\n",
    "data = [(1, 'J'), (4, 'N'), (3, 'H'), (2, 'O')]\n",
    "for item in data:\n",
    "  heappush(heap, item)\n",
    "  print(\"item \", item, \"heap \", heap)\n",
    "\n",
    "while heap:\n",
    "  print(heappop(heap)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.391674201191805"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = list(np.random.uniform(10, 15, 10))\n",
    "np.min(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.391674201191805,\n",
       " 12.592464370966416,\n",
       " 14.611942123977197,\n",
       " 14.34392951653233,\n",
       " 12.02532732428294,\n",
       " 10.543753908281914,\n",
       " 13.859416915889499,\n",
       " 10.786223147420115,\n",
       " 14.640421434377895,\n",
       " 12.629468290152852]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
