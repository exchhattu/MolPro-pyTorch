{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-implement-keras-layers-core-lambda-in-pytorch/5903\n",
    "class BestMeanPooling(nn.Module):\n",
    "    \n",
    "  def __init__(self, n_topN):\n",
    "    super(BestMeanPooling, self).__init__()\n",
    "    self.topN = n_topN\n",
    "        \n",
    "  def forward(self, aX_tr):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    aX_tr_sorted, _ = torch.sort(aX_tr, axis=1) # also return index\n",
    "    return torch.mean(aX_tr_sorted[:, -self.topN:,:], axis=1)\n",
    "    # print(\"B = \", aX_tr_sorted.size(), aX_tr_sorted)\n",
    "    # c = torch.mean(aX_tr_sorted[:, -self.topN:,:], axis=1)\n",
    "    # print(\"C = \", c.size(), c)\n",
    "    return c\n",
    "\n",
    "# Test\n",
    "aA = torch.randn(1, 200).reshape(4, 10, 5)\n",
    "mean_pool = BestMeanPooling(2) \n",
    "aA = mean_pool(aA)\n",
    "# print(aA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticData(Dataset):\n",
    "    def __init__(self, upper, x, y, z):\n",
    "      self.aX_tr_sy = np.random.random_sample(upper).reshape(x, y, z)\n",
    "      self.ay_tr_sy = np.random.randint(0, 2, x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.aX_tr_sy)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.aX_tr_sy[idx], self.ay_tr_sy[idx] #.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassCNN(nn.Module):\n",
    "\n",
    "  def __init__(self, n_in, n_out, num_event=10, topN=5, prob=0.00):\n",
    "    super(MassCNN,self).__init__()\n",
    "    self.n_filter = n_out\n",
    "\n",
    "    #first layers\n",
    "    self.conv_1        = nn.Conv1d(n_in, n_out, kernel_size=1)\n",
    "    \n",
    "    # output of the layer is # of filters\n",
    "    self.bmpool_1      = BestMeanPooling(topN)\n",
    "    self.dropout_L1    = nn.Dropout(p=prob)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    self.fcn_1 = nn.Linear(3, 2)\n",
    "    # self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, aX_tr):\n",
    "    aX_tr = self.conv_1(aX_tr)\n",
    "    aX_tr = torch.relu(aX_tr)\n",
    "    \n",
    "    # This will collapse one dimension\n",
    "    aX_tr = self.bmpool_1(aX_tr)\n",
    "    \n",
    "    if self.n_filter > 4: \n",
    "      aX_tr = self.dropout_L1(aX_tr)\n",
    "    \n",
    "    aX_tr = aX_tr.view(aX_tr.size(0), -1)\n",
    "    aX_tr = self.fcn_1(aX_tr)\n",
    "    # aX_tr = self.softmax(aX_tr)\n",
    "    return aX_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs=10, lr=0.1, filter=12, maxpool_percent=100.00, n_iter = 0):\n",
    "    \"\"\"\n",
    "      Aim:\n",
    "\n",
    "      params:\n",
    "\n",
    "      returns:\n",
    "    \"\"\"\n",
    "    # Random data\n",
    "    d_trains = SyntheticData(750, 5, 50, 3)\n",
    "    d_valids = SyntheticData(750, 5, 50, 3)\n",
    "\n",
    "    o_mass_model = MassCNN(50, filter, 3, 5)\n",
    "    \n",
    "    # Trying to remove this type casting from here.\n",
    "    # massModel = massModel #.float()\n",
    "    \n",
    "    # optimization\n",
    "    o_optimizer = torch.optim.SGD(o_mass_model.parameters(), lr=0.0001)\n",
    "    o_cost      = torch.nn.CrossEntropyLoss()\n",
    "  \n",
    "    t_cost_per_epoch = []\n",
    "    \n",
    "    # model.state_dict()['cnn1.weight']\n",
    "    # model.state_dict()['cnn2.weight']\n",
    "    af_Tr = DataLoader(d_trains, batch_size=100, shuffle=True)\n",
    "    af_Va = DataLoader(d_valids, batch_size=10, shuffle=True)\n",
    "    t_solutions = []\n",
    "    for epoch in range(n_epochs):\n",
    "      m_models = dict()\n",
    "      fgr_cost = 0\n",
    "      for af_X, af_y in af_Tr:  # iterative over batch of data\n",
    "        ## Back propagation the loss\n",
    "        o_optimizer.zero_grad()\n",
    "        af_y_pr  = o_mass_model(af_X.float())\n",
    "        o_loss   = o_cost(af_y_pr, af_y)\n",
    "        o_loss.backward()\n",
    "        o_optimizer.step() # back propagate\n",
    "        fgr_cost += o_loss.data\n",
    "        \n",
    "      t_cost_per_epoch.append(fgr_cost)\n",
    "      \n",
    "      #print(o_mass_model.state_dict().keys())\n",
    "      #print(\"Epochs in fun \", epoch, id(o_mass_model))\n",
    "      o_mass_model.eval()\n",
    "      f_correct = 0\n",
    "      for af_x_va, af_y_va in af_Va:\n",
    "        z  = o_mass_model(af_x_va.float()).data\n",
    "        _, yhat = torch.max(z.data, 1)\n",
    "        f_correct += (yhat == af_y_va).sum().item()\n",
    "      f_accuracy = \"%0.3f\" %(f_correct / 1000) # fixed this with sample size\n",
    "    \n",
    "      m_models[\"epoch\"] = epoch \n",
    "      m_models[\"iteration\"] = n_iter\n",
    "      m_models[\"val_acc\"] = f_accuracy\n",
    "      m_models[\"model_state\"] =  o_mass_model.state_dict()\n",
    "      m_models[\"optimizer_state\"] =  o_optimizer.state_dict()\n",
    "      m_models[\"loss\"] =  o_cost.state_dict()\n",
    "      m_models[\"model\"] = o_mass_model\n",
    "      t_solutions.append(m_models.copy())\n",
    "      del m_models\n",
    "    return t_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(t_models):\n",
    "  \"\"\"\n",
    "  Select the best solutions from the pool\n",
    "  \n",
    "  params: t_models - list of models. Each model is dictionary that contains parameter\n",
    "                     and hyperparameters.\n",
    "  \"\"\"\n",
    "  n_best_sol      = 5\n",
    "  t_best_val_accs = []\n",
    "  t_best_models   = []\n",
    "    \n",
    "  for o_model in t_models: \n",
    "    b_update  = True\n",
    "    f_val_acc = float(o_model[\"val_acc\"])\n",
    "    if len(t_best_val_accs) >= n_best_sol:\n",
    "      print(t_best_val_accs)\n",
    "      f_lowest_acc = np.min(t_best_val_accs)\n",
    "      if f_val_acc > f_lowest_acc:\n",
    "        i_cur_idx = t_best_val_accs.index(f_lowest_acc)\n",
    "        t_best_val_accs.pop(i_cur_idx)\n",
    "        t_best_models.pop(i_cur_idx)\n",
    "      else:\n",
    "        b_update = False\n",
    "    if b_update: \n",
    "      t_best_val_accs.append(f_val_acc)\n",
    "      t_best_models.append(o_model)\n",
    "  return t_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the multiple models for testing\n",
    "\n",
    "def test_model(d_tests, t_best_sols):\n",
    "  \"\"\"\n",
    "  Test performance of model on independent data. Since multiple models were generated, \n",
    "  these models were tested aganist an independent data and their average performance \n",
    "  will be returned as final result.\n",
    "  \n",
    "  params:\n",
    "    d_tests: data for independent test\n",
    "    t_best_sols: list of best solutions\n",
    "  \"\"\"\n",
    "  t_test_accs = []\n",
    "  for o_best_sol in t_best_sols:\n",
    "    o_best_model = o_best_sol[\"model\"]\n",
    "    o_best_model.eval()\n",
    "    f_correct = 0\n",
    "    n_test_count = len(d_tests)\n",
    "    for d_test_X, d_target_Y in d_tests:\n",
    "      z  = o_best_model(d_test_X.float()).data # remove float\n",
    "      _, yhat = torch.max(z.data, 1)\n",
    "      f_correct += (yhat == d_target_Y).sum().item()\n",
    "    f_accuracy = \"%0.3f\" %(f_correct / n_test_count) # fixed this with sample size\n",
    "    t_test_accs.append(float(f_accuracy))\n",
    "  print(\"Coding: \", t_test_accs)\n",
    "  return np.mean(t_test_accs)\n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.003]\n",
      "[0.003, 0.003, 0.003, 0.003, 0.004]\n",
      "[0.003, 0.003, 0.003, 0.004, 0.004]\n",
      "[0.003, 0.003, 0.004, 0.004, 0.004]\n",
      "[0.003, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.004]\n",
      "[0.004, 0.004, 0.004, 0.004, 0.005]\n",
      "[0.004, 0.004, 0.004, 0.005, 0.005]\n",
      "[0.004, 0.004, 0.005, 0.005, 0.005]\n",
      "[0.004, 0.005, 0.005, 0.005, 0.005]\n",
      "[0.005, 0.005, 0.005, 0.005, 0.005]\n",
      "[0.005, 0.005, 0.005, 0.005, 0.005]\n",
      "[0.005, 0.005, 0.005, 0.005, 0.005]\n",
      "[0.005, 0.005, 0.005, 0.005, 0.005]\n",
      "[0.005, 0.005, 0.005, 0.005, 0.005]\n",
      "Coding:  [0.8, 0.8, 0.8, 0.8, 0.8]\n",
      "Coding:  0.8\n"
     ]
    }
   ],
   "source": [
    "# get 20% of best solutions after iterating \n",
    "# between 15 to 30 times (these # are randomly selected)\n",
    "\n",
    "n_cell = 10\n",
    "f_maxpools    = [0.01, 1.0, 5.0, 20.0, 100.0]\n",
    "n_maxpool_len = len(f_maxpools)\n",
    "\n",
    "t_models      = []\n",
    "for n_trial in range(10):\n",
    "  f_lr       = 10 ** np.random.uniform(-3, -2)\n",
    "  i_filter   = np.random.choice(range(3,10))\n",
    "  f_max_pool = f_maxpools[n_trial%n_maxpool_len]\n",
    "  f_maxpool  = max(1, int(f_max_pool/100. * n_cell))\n",
    "  t_models += train(10, f_lr, i_filter, f_maxpool)\n",
    "\n",
    "t_best_models = get_best_models(t_models)\n",
    "\n",
    "# generate test data\n",
    "d_tests  = SyntheticData(750, 5, 50, 3)\n",
    "af_test = DataLoader(d_tests)\n",
    "predicted_Y = test_model(af_test, t_best_models)\n",
    "print(\"Coding: \", predicted_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 5)\n",
      "(None, 20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Identity:0' shape=(None, 20, 10) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Activation, Dropout\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# the input layer\n",
    "ncell = 20\n",
    "nmark = 5\n",
    "data_input = Input(shape=(ncell, nmark))\n",
    "\n",
    "nfilter = 10\n",
    "# the filters\n",
    "print(data_input.shape)\n",
    "conv = Convolution1D(nfilter, 1, activation='linear', name='conv1')(data_input)\n",
    "print(conv.shape)\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "K.print_tensor(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, stride=1, padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4])\n",
      "torch.Size([5, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "in_channel = 16\n",
    "out_channel = 33\n",
    "filter/kernal = 3\n",
    "batch = 20\n",
    "\"\"\"\n",
    "\n",
    "m = nn.Conv1d(3, 6, 2, stride=1)\n",
    "input = torch.randn(5, 3, 4)\n",
    "print(input.shape)\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6264, -1.0689,  0.8947,  0.6613],\n",
       "         [-0.0609, -0.1427,  0.6524, -0.6114],\n",
       "         [ 0.8502, -1.1811,  0.2023, -0.1698]],\n",
       "\n",
       "        [[ 1.9426, -0.3940,  0.1056, -0.0188],\n",
       "         [ 0.3037, -0.2814,  0.5930,  0.8171],\n",
       "         [-2.2596, -1.3615,  0.1173, -0.7407]],\n",
       "\n",
       "        [[ 1.2895,  0.9866,  0.4093,  0.0553],\n",
       "         [-1.3481,  2.8548,  0.0110, -1.6774],\n",
       "         [ 0.3700, -0.9349,  1.0409, -0.3617]],\n",
       "\n",
       "        [[ 0.5112, -1.8242, -1.7307,  0.7217],\n",
       "         [-0.0374,  0.0032,  0.2039, -0.5883],\n",
       "         [ 0.5928, -0.6295, -1.2091, -1.3084]],\n",
       "\n",
       "        [[-1.9813, -1.3394, -0.0769, -0.7997],\n",
       "         [ 0.2150,  1.5719, -2.7364,  0.4307],\n",
       "         [ 0.1438,  0.9363, -1.3732, -0.2300]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9761,  0.0494,  0.2616],\n",
       "         [-0.1339,  0.6016,  0.2954],\n",
       "         [-0.6942, -0.1283,  0.2529],\n",
       "         [ 0.1993, -0.4620,  0.0325],\n",
       "         [ 0.2876,  0.4290, -0.1779],\n",
       "         [-0.5350,  0.9385, -0.0227]],\n",
       "\n",
       "        [[-0.2537, -0.0465,  0.3789],\n",
       "         [-0.3155,  0.3745, -0.0300],\n",
       "         [ 0.5402, -0.1488, -0.4576],\n",
       "         [ 0.0028, -0.2816,  0.0329],\n",
       "         [ 1.2074,  0.7800,  0.0326],\n",
       "         [ 0.3150,  0.5926,  0.3196]],\n",
       "\n",
       "        [[-1.0598, -0.3572,  1.1398],\n",
       "         [-1.1925,  0.6258,  0.5860],\n",
       "         [-1.1714,  0.9764, -0.0204],\n",
       "         [-1.0419,  0.8299,  0.0927],\n",
       "         [-0.2879, -0.1128, -0.0728],\n",
       "         [ 0.7170,  0.3465, -0.6416]],\n",
       "\n",
       "        [[ 0.9160,  1.6411,  1.2504],\n",
       "         [-0.0197,  0.5581,  0.8467],\n",
       "         [-0.7527, -0.8986, -0.0225],\n",
       "         [ 0.4419,  0.2732, -0.3323],\n",
       "         [ 0.6353,  1.1966,  0.6019],\n",
       "         [-0.7396,  0.1547,  0.9562]],\n",
       "\n",
       "        [[ 0.6942,  2.6339, -0.0306],\n",
       "         [ 0.6850,  1.3378, -0.0089],\n",
       "         [-1.1177,  0.2228, -0.8021],\n",
       "         [ 0.3270,  0.6167, -0.8449],\n",
       "         [ 0.5549, -0.0820,  1.5613],\n",
       "         [ 0.1419, -0.3238,  0.0370]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item  (1, 'J') heap  [(1, 'J')]\n",
      "item  (4, 'N') heap  [(1, 'J'), (4, 'N')]\n",
      "item  (3, 'H') heap  [(1, 'J'), (4, 'N'), (3, 'H')]\n",
      "item  (2, 'O') heap  [(1, 'J'), (2, 'O'), (3, 'H'), (4, 'N')]\n",
      "J\n",
      "O\n",
      "H\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "from heapq import heappush, heappop\n",
    "heap = []\n",
    "data = [(1, 'J'), (4, 'N'), (3, 'H'), (2, 'O')]\n",
    "for item in data:\n",
    "  heappush(heap, item)\n",
    "  print(\"item \", item, \"heap \", heap)\n",
    "\n",
    "while heap:\n",
    "  print(heappop(heap)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.087687378546482"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = list(np.random.uniform(10, 15, 10))\n",
    "np.min(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.019949284190336,\n",
       " 11.087687378546482,\n",
       " 14.139058826352988,\n",
       " 13.207704703872293,\n",
       " 11.350121239849562,\n",
       " 14.330643797103484,\n",
       " 11.649265839287658,\n",
       " 12.824486314689969,\n",
       " 11.334979842012935,\n",
       " 11.6434292321006]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
